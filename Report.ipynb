{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-0624e6ba5a4c>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-0624e6ba5a4c>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    For this toy example, the problem is to solve in UNITY ML environment a banana collection task, namely collect as\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Training An Agent To Solve A Task With Rewards using a Q-Network\n",
    "\n",
    "## Algorithm \n",
    "\n",
    "For this toy example, the problem is to solve in UNITY ML environment a banana collection task, namely collect as\n",
    "many yellow bananas as possible while avoiding blue bananas.\n",
    "\n",
    "The problem is modeled as an episodic task during which the agent has to maximize the expected cumulative rewards.\n",
    "Because we choose to model the solution as a Q-Learning algorithm, the agent implements the following equation:\n",
    "\n",
    "![DQN equation](DQN_Equation.png)\n",
    "<img src=\"DQN_Equation.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
